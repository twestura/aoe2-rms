//! Tokenizer for converting lexemes to tokens.
